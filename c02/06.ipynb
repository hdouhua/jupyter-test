{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "0353003b164be0642e33afb02724bda0ba1f40f30d4d7e5996e46d8aa1b66523"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Python 协程\n",
    "\n",
    "## 什么是协程？\n",
    "\n",
    "协程（Coroutine）字面理解协作完成任务，而非线程的抢占式多任务。\n",
    "\n",
    "是实现并发编程的一种方式。一说并发，就让人想到了多线程 / 多进程模型，没错，多线程 / 多进程，正是解决并发问题的经典模型之一。最初的互联网世界，多线程 / 多进程在服务器并发中，起到举足轻重的作用。\n",
    "\n",
    "事件循环（event loop）启动一个统一的调度器，让调度器来决定一个时刻去运行哪个任务，于是*省却了多线程中启动线程、管理线程、同步锁等各种开销*。让人惊喜地发现，这种工具完美地继承了事件循环的优越性，同时还能提供 async / await 语法糖，解决了执行性和可读性共存的难题。于是，协程逐渐被更多人发现并看好。\n",
    "\n",
    "回到我们的 Python。使用生成器，是 Python 2 时代实现协程的老方法，Python 3.7 提供了新的基于 asyncio 和 async / await 的方法。我们直接来讲新方法。\n",
    "\n",
    "## 从一个爬虫说起\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "crawling url_1\n",
      "OK url_1\n",
      "crawling url_2\n",
      "OK url_2\n",
      "crawling url_3\n",
      "OK url_3\n",
      "crawling url_4\n",
      "OK url_4\n",
      "CPU times: user 3.68 ms, sys: 2.02 ms, total: 5.71 ms\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def crawl_page(url):\n",
    "    print('crawling {}'.format(url))\n",
    "    sleep_time = int(url.split('_')[-1])\n",
    "    time.sleep(sleep_time)\n",
    "    print('OK {}'.format(url))\n",
    "\n",
    "def main(urls):\n",
    "    for url in urls:\n",
    "        crawl_page(url)\n",
    "\n",
    "%time main(['url_1', 'url_2', 'url_3', 'url_4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import functools\n",
    "\n",
    "def async_time_it(func):\n",
    "    @functools.wraps(func)\n",
    "    async def wrapper(*args, **kwargs):\n",
    "        start = time.perf_counter()\n",
    "        res = await func(*args, **kwargs)\n",
    "        end = time.perf_counter()\n",
    "        print(f'\\nTook {(end - start):.3f} s')\n",
    "        return res\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "crawling url_1\n",
      "OK url_1\n",
      "crawling url_2\n",
      "OK url_2\n",
      "crawling url_3\n",
      "OK url_3\n",
      "crawling url_4\n",
      "OK url_4\n",
      "\n",
      "Took 10.008 s\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async def crawl_page(url):\n",
    "    print('crawling {}'.format(url))\n",
    "    sleep_time = int(url.split('_')[-1])\n",
    "    await asyncio.sleep(sleep_time)\n",
    "    print('OK {}'.format(url))\n",
    "\n",
    "async def main(urls):\n",
    "    for url in urls:\n",
    "        await crawl_page(url)\n",
    "\n",
    "@async_time_it\n",
    "async def run():\n",
    "    await main(['url_1', 'url_2', 'url_3', 'url_4'])\n",
    "\n",
    "await run()"
   ]
  },
  {
   "source": [
    "## 协程的执行\n",
    "\n",
    "执行协程有多种方法，这里我介绍一下常用的三种。\n",
    "\n",
    "- 首先，我们可以通过 await 来调用。await 执行的效果，和 Python 正常执行是一样的，也就是说程序会阻塞在这里，进入被调用的协程函数，执行完毕返回后再继续，而这也是 await 的字面意思。\n",
    "\n",
    "   代码中 await asyncio.sleep(sleep_time) 会在这里休息若干秒，await crawl_page(url) 则会执行 crawl_page() 函数。\n",
    "   \n",
    "- 其次，我们可以通过 asyncio.create_task() 来创建任务。\n",
    "- 最后，我们需要 asyncio.run 来触发运行。asyncio.run 这个函数是 Python 3.7 之后才有的特性，可以让 Python 的协程接口变得非常简单，你不用去理会事件循环怎么定义和怎么使用的问题。一个非常好的编程规范是，asyncio.run(main()) 作为主程序的入口函数，在程序运行周期内，只调用一次 asyncio.run。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "crawling url_1\n",
      "crawling url_2\n",
      "crawling url_3\n",
      "crawling url_4\n",
      "OK url_1\n",
      "OK url_2\n",
      "OK url_3\n",
      "OK url_4\n",
      "\n",
      "Took 4.001 s\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async def crawl_page(url):\n",
    "    print('crawling {}'.format(url))\n",
    "    sleep_time = int(url.split('_')[-1])\n",
    "    await asyncio.sleep(sleep_time)\n",
    "    print('OK {}'.format(url))\n",
    "\n",
    "async def main(urls):\n",
    "    tasks = [asyncio.create_task(crawl_page(url)) for url in urls]\n",
    "    for task in tasks:\n",
    "        await task\n",
    "\n",
    "@async_time_it\n",
    "async def run():\n",
    "    await main(['url_1', 'url_2', 'url_3', 'url_4'])\n",
    "\n",
    "await run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "before await\n",
      "worker_1 start\n",
      "worker_1 done\n",
      "awaited worker_1\n",
      "worker_2 start\n",
      "worker_2 done\n",
      "awaited worker_2\n",
      "\n",
      "Took 3.002 s\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async def worker_1():\n",
    "    print('worker_1 start')\n",
    "    await asyncio.sleep(1)\n",
    "    print('worker_1 done')\n",
    "\n",
    "async def worker_2():\n",
    "    print('worker_2 start')\n",
    "    await asyncio.sleep(2)\n",
    "    print('worker_2 done')\n",
    "\n",
    "@async_time_it\n",
    "async def main():\n",
    "    print('before await')\n",
    "    await worker_1()\n",
    "    print('awaited worker_1')\n",
    "    await worker_2()\n",
    "    print('awaited worker_2')\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "before await\n",
      "worker_1 start\n",
      "worker_2 start\n",
      "worker_1 done\n",
      "awaited worker_1\n",
      "worker_2 done\n",
      "awaited worker_2\n",
      "\n",
      "Took 2.003 s\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async def worker_1():\n",
    "    print('worker_1 start')\n",
    "    await asyncio.sleep(1)\n",
    "    print('worker_1 done')\n",
    "\n",
    "async def worker_2():\n",
    "    print('worker_2 start')\n",
    "    await asyncio.sleep(2)\n",
    "    print('worker_2 done')\n",
    "\n",
    "@async_time_it\n",
    "async def main():\n",
    "    task1 = asyncio.create_task(worker_1())\n",
    "    task2 = asyncio.create_task(worker_2())\n",
    "\n",
    "    print('before await')\n",
    "    await task1\n",
    "    print('awaited worker_1')\n",
    "    await task2\n",
    "    print('awaited worker_2')\n",
    "\n",
    "await main()"
   ]
  },
  {
   "source": [
    "让我们来看一下上面的代码都做了些什么？\n",
    "\n",
    "1. await main()，程序进入 main() 函数，事件循环开启；\n",
    "1. task1 和 task2 任务被创建，并进入事件循环等待运行；\n",
    "1. 运行到 print，输出 'before await'；\n",
    "1. await task1 执行，用户选择从当前的主任务中切出，事件调度器开始调度 worker_1；\n",
    "1. worker_1 开始运行，运行 print 输出'worker_1 start'，然后运行到 await asyncio.sleep(1)， 从当前任务切出，事件调度器开始调度 worker_2；\n",
    "1. worker_2 开始运行，运行 print 输出 'worker_2 start'，然后运行 await asyncio.sleep(2) 从当前任务切出；\n",
    "\n",
    "   以上所有事件的运行时间，都应该在 1ms 到 10ms 之间，甚至可能更短，事件调度器从这个时候开始暂停调度；\n",
    "\n",
    "1. 一秒钟后，worker_1 的 sleep 完成，事件调度器将控制权重新传给 task_1，输出 'worker_1 done'，task_1 完成任务，从事件循环中退出；\n",
    "1. await task1 完成，事件调度器将控制器传给主任务，输出 'awaited worker_1'，然后在 await task2 处继续等待；\n",
    "1. 两秒钟后，worker_2 的 sleep 完成，事件调度器将控制权重新传给 task_2，输出 'worker_2 done'，task_2 完成任务，从事件循环中退出；\n",
    "1. 主任务输出 'awaited worker_2'，协程全任务结束，事件循环结束。\n",
    "\n",
    "如果我们想给某些协程任务限定运行时间，一旦超时就取消，又该怎么做呢？再进一步，如果某些协程运行时出现错误，又该怎么处理呢？\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, ZeroDivisionError('division by zero'), CancelledError()]\n\nTook 2.002 s\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async def worker_1():\n",
    "    await asyncio.sleep(1)\n",
    "    return 1\n",
    "\n",
    "async def worker_2():\n",
    "    await asyncio.sleep(2)\n",
    "    return 2 / 0\n",
    "\n",
    "async def worker_3():\n",
    "    await asyncio.sleep(3)\n",
    "    return 3\n",
    "\n",
    "@async_time_it\n",
    "async def main():\n",
    "    task_1 = asyncio.create_task(worker_1())\n",
    "    task_2 = asyncio.create_task(worker_2())\n",
    "    task_3 = asyncio.create_task(worker_3())\n",
    "\n",
    "    # 这种写法比较特别：2 秒后取消 task_3。如果那时 task_3 已经完成，这个语句相当于无效语句\n",
    "    await asyncio.sleep(2)\n",
    "    task_3.cancel()\n",
    "\n",
    "    # 很关键的 return_exceptions=True，避免直接抛出异常到主线程\n",
    "    res = await asyncio.gather(task_1, task_2, task_3, return_exceptions=True)\n",
    "    print(res)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "source": [
    "注意 `return_exceptions=True` 这行代码。如果不设置这个参数，错误就会完整地 throw 到我们这个执行层，从而需要 try except 来捕捉，这也就意味着其他还没被执行的任务会被全部取消掉。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> producer_1 put a val: 7\n",
      ">> producer_2 put a val: 8\n",
      "<< consumer_1 get a val: 7\n",
      ">> producer_1 put a val: 10\n",
      ">> producer_2 put a val: 5\n",
      "<< consumer_1 get a val: 8\n",
      ">> producer_1 put a val: 7\n",
      ">> producer_2 put a val: 4\n",
      "<< consumer_1 get a val: 10\n",
      ">> producer_1 put a val: 4\n",
      ">> producer_2 put a val: 5\n",
      "<< consumer_1 get a val: 5\n",
      ">> producer_1 put a val: 2\n",
      ">> producer_2 put a val: 5\n",
      "<< consumer_1 get a val: 7\n",
      "<< consumer_1 get a val: 4\n",
      "<< consumer_1 get a val: 4\n",
      "<< consumer_1 get a val: 5\n",
      "<< consumer_1 get a val: 2\n",
      "<< consumer_1 get a val: 5\n",
      "\n",
      "Took 10.002 s\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import random\n",
    "\n",
    "async def consumer(queue, id):\n",
    "    while True:\n",
    "        val = await queue.get()\n",
    "        print('<< {} get a val: {}'.format(id, val))\n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "async def producer(queue, id):\n",
    "    for i in range(5):\n",
    "        val = random.randint(1, 10)\n",
    "        await queue.put(val)\n",
    "        print('>> {} put a val: {}'.format(id, val))\n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "@async_time_it\n",
    "async def main():\n",
    "    queue = asyncio.Queue()\n",
    "\n",
    "    consumer_1 = asyncio.create_task(consumer(queue, 'consumer_1'))\n",
    "\n",
    "    producer_1 = asyncio.create_task(producer(queue, 'producer_1'))\n",
    "    producer_2 = asyncio.create_task(producer(queue, 'producer_2'))\n",
    "\n",
    "    # 10 秒后取消消费队列\n",
    "    await asyncio.sleep(10)\n",
    "    consumer_1.cancel()\n",
    "    \n",
    "    await asyncio.gather(consumer_1, producer_1, producer_2, return_exceptions=True)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "source": [
    "## 实战：豆瓣近日推荐电影爬虫\n",
    "\n",
    "任务描述：\n",
    "\n",
    "https://movie.douban.com/cinema/later/beijing/ 这个页面描述了北京最近上映的电影，通过 Python 得到这些电影的名称、上映时间和海报。\n",
    "\n",
    "\n",
    "需要预安装 requests、bs4 和 lxml\n",
    "\n",
    "```shell\n",
    "pip3 install requests bs4 lxml aiohttp\n",
    "```\n",
    "\n",
    ">参考\n",
    ">\n",
    ">- [requests](https://requests.readthedocs.io/en/master/user/quickstart/)\n",
    ">- [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/)\n",
    ">- [aiohttp](https://docs.aiohttp.org/en/stable/)\n",
    "\n",
    "- 同步版代码\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "应承 11月26日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2626077915.jpg\n",
      "妈妈不再摇滚 11月26日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2615614077.jpg\n",
      "一秒钟 11月27日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2623809595.jpg\n",
      "疯狂原始人2 11月27日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2624607255.jpg\n",
      "日光之下 11月27日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2625833086.jpg\n",
      "闺蜜心窍 11月27日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2623348372.jpg\n",
      "隐形人 12月04日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2624870548.jpg\n",
      "赤狐书生 12月04日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2622753154.jpg\n",
      "怪物猎人 12月04日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2625920629.jpg\n",
      "如果声音不记得 12月04日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2557141890.jpg\n",
      "宝可梦：超梦的逆袭 进化 12月04日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2625913008.jpg\n",
      "五彩缤纷 12月04日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2579515502.jpg\n",
      "棒！少年 12月11日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2626104935.jpg\n",
      "沐浴之王 12月11日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2623865439.jpg\n",
      "哆啦A梦：大雄的新恐龙 12月11日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2624375776.jpg\n",
      "少女佳禾 12月11日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2571245699.jpg\n",
      "灌篮高手的契约 12月11日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2624831861.jpg\n",
      "日不落酒店 12月12日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2619899048.jpg\n",
      "小美人鱼的奇幻冒险 12月12日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2614997193.jpg\n",
      "神奇女侠1984 12月18日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2626281813.jpg\n",
      "CPU times: user 838 ms, sys: 42 ms, total: 880 ms\n",
      "Wall time: 22.6 s\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "RES_URL = \"https://movie.douban.com/cinema/later/beijing/\"\n",
    "HEADERS = {'user-agent': 'demo/0.9.0'}\n",
    "\n",
    "# 为了方便使用 BeautifulSoup 分析，以下代码可以先存储 HTTP 请求结果\n",
    "# res = requests.get(RES_URL, headers=HEADERS)\n",
    "# init_page = res.content\n",
    "# 存储\n",
    "# %store init_page\n",
    "# 加载\n",
    "# %store -r init_page\n",
    "# print(len(init_page))\n",
    "\n",
    "def main():\n",
    "    res = requests.get(RES_URL, headers=HEADERS)\n",
    "    init_page = res.content\n",
    "    init_soup = BeautifulSoup(init_page, 'lxml')\n",
    "    all_movies = init_soup.find('div', id=\"showing-soon\")\n",
    "    for each_movie in all_movies.find_all('div', class_=\"item\"):\n",
    "        all_a_tag = each_movie.find_all('a')\n",
    "        all_li_tag = each_movie.find_all('li')\n",
    "        \n",
    "        movie_name = all_a_tag[1].text\n",
    "        url_to_fetch = all_a_tag[1]['href']\n",
    "        movie_date = all_li_tag[0].text\n",
    "\n",
    "        # all_img_tag = each_movie.find_all('img')\n",
    "        # img_tag = all_img_tag[0]\n",
    "\n",
    "        # 本可以直接使用上面代码获取图片，此处故意增加 HTTP 请求\n",
    "        response_item = requests.get(url_to_fetch, headers=HEADERS).content\n",
    "        soup_item = BeautifulSoup(response_item, 'lxml')\n",
    "        img_tag = soup_item.find('img')\n",
    "\n",
    "        print('{} {} {}'.format(movie_name, movie_date, img_tag['src']))\n",
    "\n",
    "%time main()"
   ]
  },
  {
   "source": [
    "- 异步版代码"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "应承 11月26日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2626077915.jpg\n",
      "妈妈不再摇滚 11月26日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2615614077.jpg\n",
      "一秒钟 11月27日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2623809595.jpg\n",
      "疯狂原始人2 11月27日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2624607255.jpg\n",
      "日光之下 11月27日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2625833086.jpg\n",
      "闺蜜心窍 11月27日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2623348372.jpg\n",
      "隐形人 12月04日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2624870548.jpg\n",
      "赤狐书生 12月04日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2622753154.jpg\n",
      "怪物猎人 12月04日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2625920629.jpg\n",
      "如果声音不记得 12月04日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2557141890.jpg\n",
      "宝可梦：超梦的逆袭 进化 12月04日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2625913008.jpg\n",
      "五彩缤纷 12月04日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2579515502.jpg\n",
      "棒！少年 12月11日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2626104935.jpg\n",
      "沐浴之王 12月11日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2623865439.jpg\n",
      "哆啦A梦：大雄的新恐龙 12月11日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2624375776.jpg\n",
      "少女佳禾 12月11日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2571245699.jpg\n",
      "灌篮高手的契约 12月11日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2624831861.jpg\n",
      "日不落酒店 12月12日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2619899048.jpg\n",
      "小美人鱼的奇幻冒险 12月12日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2614997193.jpg\n",
      "神奇女侠1984 12月18日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2626281813.jpg\n",
      "\n",
      "Took 3.899 s\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "RES_URL = \"https://movie.douban.com/cinema/later/beijing/\"\n",
    "HEADERS = {'user-agent': 'demo/0.9.0'}\n",
    "DEBUG = False\n",
    "\n",
    "\n",
    "async def fetch_content(url, headers=HEADERS):\n",
    "    async with aiohttp.ClientSession(\n",
    "        headers=headers, connector=aiohttp.TCPConnector(ssl=False)\n",
    "    ) as session:\n",
    "        async with session.get(url) as response:\n",
    "            if DEBUG:\n",
    "                print(\"Status:\", response.status)\n",
    "                print(\"Content-type:\", response.headers['content-type'])\n",
    "            return await response.text()\n",
    "\n",
    "@async_time_it\n",
    "async def main():\n",
    "    init_page = await fetch_content(RES_URL)\n",
    "    init_soup = BeautifulSoup(init_page, 'lxml')\n",
    "\n",
    "    movie_names, urls_to_fetch, movie_dates = [], [], []\n",
    "\n",
    "    all_movies = init_soup.find('div', id=\"showing-soon\")\n",
    "    for each_movie in all_movies.find_all('div', class_=\"item\"):\n",
    "        all_a_tag = each_movie.find_all('a')\n",
    "        all_li_tag = each_movie.find_all('li')\n",
    "\n",
    "        movie_names.append(all_a_tag[1].text)\n",
    "        urls_to_fetch.append(all_a_tag[1]['href'])\n",
    "        movie_dates.append(all_li_tag[0].text)\n",
    "\n",
    "    tasks = [fetch_content(url) for url in urls_to_fetch]\n",
    "    pages = await asyncio.gather(*tasks)\n",
    "\n",
    "    for movie_name, movie_date, page in zip(movie_names, movie_dates, pages):\n",
    "        soup_item = BeautifulSoup(page, 'lxml')\n",
    "        img_tag = soup_item.find('img')\n",
    "\n",
    "        print('{} {} {}'.format(movie_name, movie_date, img_tag['src']))\n",
    "\n",
    "await main()"
   ]
  },
  {
   "source": [
    "## 总结\n",
    "\n",
    "- 协程和多线程的区别，主要在于两点，一是协程为单线程；二是协程由用户决定，在哪些地方交出控制权，切换到下一个任务。\n",
    "- 协程的写法更加简洁清晰，把 async / await 语法和 create_task 结合来用，对于中小级别的并发需求已经毫无压力。\n",
    "- 写协程程序的时候，脑海中要有清晰的事件循环概念，知道程序在什么时候需要暂停、等待 I/O，什么时候需要一并执行到底。\n",
    "\n",
    "## 思考题\n",
    "\n",
    "协程怎么实现回调函数呢？\n",
    "\n",
    "TBD"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}